backend_name: llama.cpp
description: |
  llama.cpp (https://github.com/ggml-org/llama.cpp) is a lightweight, high-performance inference engine for running large language models locally.
  It focuses on efficient CPU and edge deployment with minimal dependencies, supporting quantization and broad hardware compatibility.
  It enables fast, low-cost LLM inference without requiring GPUs or complex infrastructure.
  Only models in the GGUF format are supported.
version_configs:
  cuda:
    image_name: ghcr.io/ggml-org/llama.cpp:server-cuda
    custom_framework: cuda
  cpu:
    image_name: ghcr.io/ggml-org/llama.cpp:server
    custom_framework: cpu
  vulkan:
    image_name: ghcr.io/ggml-org/llama.cpp:server-vulkan
    custom_framework: rocm
  musa:
    image_name: ghcr.io/ggml-org/llama.cpp:server-musa
    custom_framework: musa
  rocm:
    image_name: ghcr.io/ggml-org/llama.cpp:server-rocm
    custom_framework: rocm
default_version: cpu
default_run_command: -m {{model_path}} --host 0.0.0.0 --port {{port}} --alias {{model_name}}
is_built_in: false
health_check_path: /v1/models
framework_index_map:
  cuda:
    - cuda
  cpu:
    - cpu
  rocm:
    - rocm
    - vulkan
  musa:
    - musa
