# ----------------------------------------
# GPUStack Community Inference Backend Spec
# ----------------------------------------
# This file defines how a backend integrates with GPUStack.
# It is required for all community inference backends.
#
# Notes:
# - version_configs defines one or more runnable backend versions
# - GPUStack will NOT validate model correctness or inference quality
# ----------------------------------------

# Required.
# Backend identifier shown in GPUStack.
backend_name: vLLM-custom

# Optional but recommended.
# Short description of this backend and its purpose.
description: Custom vLLM backend

# Optional but recommended.
# Default backend version used when deploying without explicit selection.
default_version: v0.15.1

# Optional.
# HTTP path used by GPUStack to check backend health.
health_check_path: /v1/models

# Optional.
# Used to override the imageâ€™s default ENTRYPOINT.
default_entrypoint: >
  vllm serve

# Optional but recommended.
# Default command template to start the backend.
# Available variables:
#   {{model_path}}, {{port}}, {{worker_ip}}, {{model_name}}
default_run_command: >
  {{model_path}}
  --port {{port}}
  --host {{worker_ip}}
  --served-model-name {{model_name}}

# Optional.
# Default backend parameters injected by GPUStack.
default_backend_param:
  - --trust-remote-code

# Optional.
# Set default environment variables that are visible and editable during deployment.
default_env:
  NCCL_DEBUG: INFO

# Required.
# Defines backend versions and their runtime configuration.
version_configs:
  v0.15.1:
    # Required.
    # Container image used for this backend version.
    image_name: vllm/vllm-openai:v0.15.1

    # Optional.
    # Override the entrypoint if required and no default entrypoint is set..
    entrypoint: >
      vllm serve

    # Optional.
    # Command to run inside the container.
    # If empty, default_run_command will be used.
    run_command: >
      {{model_path}}
      --port {{port}}
      --host {{worker_ip}}
      --served-model-name {{model_name}}

    # Required.
    # Hardware / framework type.
    # Choose one of:
    #   cuda | rocm | cann | dtk | musa | corex | maca | neuware | hggc | cpu
    custom_framework: cuda

  nightly:
    image_name: vllm/vllm-openai:nightly
    entrypoint:
    run_command:
    custom_framework: cuda
